{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract atomic elements from PDF using Unstructured\"\"\"\n",
    "    print(f\"Partitioning document...{file_path}\")\n",
    "\n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Image\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "\n",
    "    print(f\"Extracted {len(elements)} elements\")\n",
    "    return elements\n",
    "\n",
    "\n",
    "file_path = \"./docs/attention.pdf\"\n",
    "elements = partition_document(file_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[2].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [element for element in elements if element.category == \"Image\"]\n",
    "print(f\"found {len(images)} images\")\n",
    "\n",
    "images[0].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [element for element in elements if element.category == \"Table\"]\n",
    "print(f\"found {len(tables)} tables\")\n",
    "\n",
    "tables[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create chunks using title based strategy\"\"\"\n",
    "    print(\"Creating chunks by title...\")\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=2400,\n",
    "        combine_text_under_n_chars=500\n",
    "    )\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Check for tables and images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "\n",
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatOllama(model=\"gemma3:4b\", temperature=0)\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to the message\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary\n",
    "\n",
    "\n",
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\"üß† Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     ‚Üí Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     ‚Üí AI summary created successfully\")\n",
    "                print(f\"     ‚Üí Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     ‚Üí Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents\n",
    "\n",
    "\n",
    "# Process chunks with AI\n",
    "processed_chunks = summarise_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf52c32",
   "metadata": {},
   "source": [
    "### Testing Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunks_to_json(chunks, filename=\"chunks_export.json\"):\n",
    "    \"\"\"Export processed chunks to clean JSON format\"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"enhanced_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"original_content\": json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "            }\n",
    "        }\n",
    "        export_data.append(chunk_data)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported {len(export_data)} chunks to {filename}\")\n",
    "    return export_data\n",
    "\n",
    "# Export your chunks\n",
    "json_data = export_chunks_to_json(processed_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3bc93",
   "metadata": {},
   "source": [
    "### Create Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents, persist_directory=\"dbv1/chroma_db\"):\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"üîÆ Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore\n",
    "\n",
    "# Create the vector store\n",
    "db = create_vector_store(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb64571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your retrieval\n",
    "query = \"What are the two main components of the Transformer architecture? \"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "# Export to JSON\n",
    "export_chunks_to_json(chunks, \"rag_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f0ace",
   "metadata": {},
   "source": [
    "### Ingestion Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb37c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_ingestion_pipeline(pdf_path: str):\n",
    "    \"\"\"Run the complete RAG ingestion pipeline\"\"\"\n",
    "    print(\"üöÄ Starting RAG Ingestion Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Partition\n",
    "    elements = partition_document(pdf_path)\n",
    "    \n",
    "    # Step 2: Chunk\n",
    "    chunks = create_chunks_by_title(elements)\n",
    "    \n",
    "    # Step 3: AI Summarisation\n",
    "    summarised_chunks = summarise_chunks(chunks)\n",
    "    \n",
    "    # Step 4: Vector Store\n",
    "    db = create_vector_store(summarised_chunks, persist_directory=\"dbv2/chroma_db\")\n",
    "    \n",
    "    print(\"üéâ Pipeline completed successfully!\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = run_complete_ingestion_pipeline(\"./docs/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb6146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many attention heads does the Transformer use, and what is the dimension of each head? \"\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "def generate_final_answer(chunks, query):\n",
    "    \"\"\"Generate final answer using multimodal content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatOllama(model=\"gemma3:4b\", temperature=0)\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"Based on the following documents, please answer this question: {query}\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"--- Document {i+1} ---\\n\"\n",
    "            \n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                \n",
    "                # Add raw text\n",
    "                raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "                \n",
    "                # Add tables as HTML\n",
    "                tables_html = original_data.get(\"tables_html\", [])\n",
    "                if tables_html:\n",
    "                    prompt_text += \"TABLES:\\n\"\n",
    "                    for j, table in enumerate(tables_html):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "            \n",
    "            prompt_text += \"\\n\"\n",
    "        \n",
    "        prompt_text += \"\"\"\n",
    "Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add all images from all chunks\n",
    "        for chunk in chunks:\n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                images_base64 = original_data.get(\"images_base64\", [])\n",
    "                \n",
    "                for image_base64 in images_base64:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                    })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Answer generation failed: {e}\")\n",
    "        return \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "# Usage\n",
    "final_answer = generate_final_answer(chunks, query)\n",
    "print(query, final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
